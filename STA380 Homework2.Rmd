---
title: "Untitled"
output: 
  html_document: 
    keep_md: yes
---

#1. Flights at ABIA

Agenda : Plot a visual story with respect to delays at Austin Airport
Dataset : Only outbound flights

Cleaning the data and getting it in the required format
```{r}
library(ggplot2)

#Reading the file
flights <- read.csv("ABIA.csv")
#names(flights)
#unique(flights$UniqueCarrier)

#Creating variables
flights$DepHour <- round(flights$CRSDepTime/100,0)
flights$Season <- ifelse(flights$Month %in% c(12,1,2),"Winter",ifelse(flights$Month %in% c(3,4,5),"Spring",ifelse(flights$Month %in% c(6,7,8),"Summer","Fall")))
flights$dummy <- 1

#Converting numeric to factors
flights$Month <- as.factor(flights$Month)
flights$DayofMonth <- as.factor(flights$DayofMonth)
flights$DayOfWeek <- as.factor(flights$DayOfWeek)

#Subsetting all flights flying out of Austin
outbound_Aus <- subset(flights, flights$Origin == "AUS")

#Subsetting all flights flying out of Austin and have experienced delays
outbound_Aus_delay <- subset(flights, flights$Origin == "AUS" & flights$DepDelay > 0)
```

Which Airline carriers are notorious for delay in departure?

```{r}
#Airlines delay count
carrier.delay <- aggregate(outbound_Aus_delay$dummy,by=list(outbound_Aus_delay$UniqueCarrier),FUN = sum, na.rm=TRUE)
names(carrier.delay)[1] <- "UniqueCarrier"
names(carrier.delay)[2] <- "AnnualDelays"

#Airlines total outbound count
carrier.total <- aggregate(outbound_Aus$dummy,by=list(outbound_Aus$UniqueCarrier),FUN = sum, na.rm=TRUE)
names(carrier.total)[1] <- "UniqueCarrier"
names(carrier.total)[2] <- "AnnualFlights"

carrier.delay.perc <- merge(carrier.total,carrier.delay, by = "UniqueCarrier")
carrier.delay.perc$per.delays <- round(carrier.delay.perc$AnnualDelays/carrier.delay.perc$AnnualFlights,3)*100

ggplot(data=carrier.delay.perc, aes(x= UniqueCarrier, y= per.delays)) + geom_point( size= (carrier.delay.perc$per.delays)/4, shape=21, fill= "white") + scale_size_area() + ggtitle("Percentage of Flights delayed by Carrier") + theme(plot.title = element_text(lineheight=1.2, face="bold")) + xlab("Airlines - Carrier") + ylab("Percentage of flights delayed")
```

Southwest Airlines (WN), EVA Air(EV) and Delta airlines(DL) had maximum percentage of flights delayed in 2008,while US Airways (US),Endeavor Air(9E) and Mesa Airlines (YV) had least percentage of flights delayed.

Now let's explore the destinations to which the carriers delay the flight the most:

```{r}
#Airlines delay count
dest.delay <- aggregate(outbound_Aus_delay$dummy,by=list(outbound_Aus_delay$Dest),FUN = sum, na.rm=TRUE)
names(dest.delay)[1] <- "Destination"
names(dest.delay)[2] <- "AnnualDelays"

dest.delay.time <- aggregate(outbound_Aus_delay$DepDelay,by=list(outbound_Aus_delay$Dest),FUN = mean, na.rm=TRUE)
names(dest.delay.time)[1] <- "Destination"
names(dest.delay.time)[2] <- "AverageDelayTime"

#Airlines total outbound count
dest.total <- aggregate(outbound_Aus$dummy,by=list(outbound_Aus$Dest),FUN = sum, na.rm=TRUE)
names(dest.total)[1] <- "Destination"
names(dest.total)[2] <- "AnnualFlights"

dest.delay.perc <- merge(dest.total,dest.delay, by = "Destination")
dest.delay.perc$per.delays <-round(dest.delay.perc$AnnualDelays/dest.delay.perc$AnnualFlights,3)*100

ggplot(data=dest.delay.perc, aes(x= Destination, y= per.delays)) + geom_point( size= dest.delay.perc$per.delays/4, shape=21, fill= "white") + scale_size_area() + ggtitle("Percentage of Flights delayed to a destination") + theme(plot.title = element_text(lineheight=1.2, face="bold")) + xlab("Destination") + ylab("Percentage of flights delayed")
```

There was single flight scheduled to Des Moines International Airport, Iowa in 2008 which was delayed. Flight to Oakland International Airport,Calfornia (236 flights scheduled out of Austin in 2008) were delayed 64% of times, while flights to Will Rogers World Airport in Oklahoma (88 scheduled flights) were also delayed ~65% times. Likewise, we observe a lot of destinations with very fewer scheduled flights and longer departure delays.

Hence, to avoid this bias,let's have a look at the delay density with respect to flights to frequent destinations.

We shall look at destinations which had over 579 flights scheduled to departure in 2008. 579 is the median of scheduled flights to any destination out of Austin in that year.

```{r}
#Only frequent destinations
ggplot(data=subset(dest.delay.perc,dest.delay.perc$AnnualFlights> median(dest.delay.perc$AnnualFlights)), aes(x= Destination, y= per.delays)) + geom_point( size= subset(dest.delay.perc,dest.delay.perc$AnnualFlights> median(dest.delay.perc$AnnualFlights))$per.delays/5 , shape=21, fill= "white") + scale_size_area() + ggtitle("Percentage of Flights delayed to Frequent destinations") + theme(plot.title = element_text(lineheight=1.2, face="bold")) + xlab("Destination") + ylab("Percentage of flights delayed")
```

We observe that over 50% scheduled flights to Baltimore and San Diego (Frequent destinations) were delayed.

Now,let's observe the destinations affected the most by delay with respect to delay time

```{r}
#install.packages("ggmap", dependencies = TRUE)
#Read airport codes
airport.codes <- read.csv("Airport_Codes_V1.csv")

#https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat

#names(airport.codes)
names(airport.codes)[4] <- "Destination"
#names(dest.delay.perc)

#Merge it to our dataset
delay.map <- merge(dest.delay.perc,airport.codes,by = "Destination" ,x.all= TRUE)
delay.map <- merge(delay.map,dest.delay.time, by = "Destination" ,x.all= TRUE)

#sapply(delay.map,class)
#sum(is.na(delay.map))
       
library(ggmap)

# getting the map
mapgilbert <- get_map(location = c(lon = mean(delay.map$Longitude), lat = mean(delay.map$Latitude)), zoom = 4,maptype = "roadmap", scale = 2)

# plotting the map with some points on it
ggmap(mapgilbert) + geom_point(data = delay.map, aes(x = Longitude, y = Latitude, fill = "red", alpha = 0.8), size = sqrt(delay.map$AverageDelayTime), shape = 21) + guides(fill=FALSE, alpha=FALSE, size=FALSE) + labs(title="Average Departure delay to Destination Airports", x="Longitude", y="Latitude") + theme(plot.title = element_text(hjust = 0, vjust = 1, face = c("bold")))
```

We notice that the flight to Iowa was delayed the most. But again, this is an outlier case due to a single scheduled flight. Hence,lets look at destinations with frequent flights out of Austin.

```{r}
#Only frequent destinations
ggplot(data=subset(delay.map,delay.map$AnnualFlights> median(delay.map$AnnualFlights)), aes(x= Destination, y= AverageDelayTime )) + geom_point( size= subset(delay.map,delay.map$AnnualFlights> median(delay.map$AnnualFlights))$AverageDelayTime/5 , shape=21, fill= "white") + scale_size_area() + ggtitle("Average departure delay to Frequent destinations") + theme(plot.title = element_text(lineheight=1.2, face="bold")) + xlab("Destination") + ylab("Percentage of flights delayed")

# getting the map
map <- get_map(location = c(lon = mean(delay.map$Longitude), lat = mean(delay.map$Latitude)), zoom = 4,maptype = "roadmap", scale = 2)

# plotting the map with some points on it
ggmap(map) + geom_point(data = subset(delay.map,delay.map$AnnualFlights> median(delay.map$AnnualFlights)), aes(x = Longitude, y = Latitude, fill = "red", alpha = 0.8), size = subset(delay.map,delay.map$AnnualFlights> median(delay.map$AnnualFlights))$AverageDelayTime/4, shape = 21) + guides(fill=FALSE, alpha=FALSE, size=FALSE) + labs(title="Averge departure delay time to destinations with frequent flights from Austin", x="Longitude", y="Latitude") + theme(plot.title = element_text(hjust = 0, vjust = 1, face = c("bold")))

```

Flights to Washington Dulles International Airport, Virginia and Newark Liberty International Airport, New Jersey were worst hit with respect to departure delay in 2008. They were closely followed by flights to O'Hare International Airport, Chicago.

```{r, echo = FALSE}
#?get_map
#----#

#map <- get_map(location = 'USA', zoom = 4)
#mapPoints <- ggmap(map) + geom_point(aes(x = delay.map$Longitude, y = delay.map$Latitude,data = delay.map, alpha = .5)) + expand_limits(x = delay.map$Longitude, y = delay.map$Latitude)

#map1 <- ggmap(map, extent='panel', base_layer=ggplot(delay.map, aes(x=delay.map$Longitude, y=delay.map$Latitude)))
#map.airports <- map1 + geom_point(color = "blue", size = sqrt(delay.map$AverageDelayTime)) + labs(title="Destiation Airports", x="Longitude", y="Latitude") + theme(plot.title = element_text(hjust = 0, vjust = 1, face = c("bold")))

#print(map.airports)

```

Looking at time plots: Which is the worst time to fly out of Austin?

```{r}
#Trends
by_hour <-aggregate(outbound_Aus_delay$DepDelay,by=list(outbound_Aus_delay$DepHour), FUN=mean, na.rm=TRUE)
#names(by_DOW_hour)
names(by_hour)[1] <- "Departure.Hour"
names(by_hour)[2] <- "Avg.Delay"

by_hour <- by_hour[order(by_hour$Departure.Hour),]

#Remove departure hour 1
by_hour <- subset(by_hour,by_hour$Departure.Hour != 1)

ggplot(data= by_hour, aes(x=Departure.Hour, y=Avg.Delay)) + geom_line() + geom_point( size=4, shape=21, fill="white") + ggtitle("Delay time by Scheduled Departure Time") + theme(plot.title = element_text(lineheight=1.2, face="bold"))
```

We observe that delay in departure gradually increases from 6 AM to 10 AM and drops till 2PM and increases it again till 4:30PM. Departure delay time starts dropping after 5PM till 10PM. Hence best tim to fly would be early in the morning or late in the night.

Looking at an hourly density plot for departure delays:

```{r}
#Density plot for departure delay wrt scheduled departure 
b1<-ggplot(outbound_Aus_delay, aes(DepHour, DepDelay)) + ylim(0, 600) +
  geom_jitter(alpha=I(1/4), col = "blue") +
  theme(legend.position = "none") +
  scale_x_continuous(breaks=seq(5,23)) +
  labs(x="Hour of Day",y="Departure Delay",title="Flight Delays by Departure Time")
b1
```

Diving into the data further, 

```{r}
#Trends by DOW
by_DOW_hour <-aggregate(outbound_Aus_delay$DepDelay,by=list(outbound_Aus_delay$DayOfWeek,outbound_Aus_delay$DepHour), FUN=mean, na.rm=TRUE)
#names(by_DOW_hour)
names(by_DOW_hour)[1] <- "DOW"
names(by_DOW_hour)[2] <- "Departure.Hour"
names(by_DOW_hour)[3] <- "Avg.Delay"

by_DOW_hour <- by_DOW_hour[order(by_DOW_hour$DOW,by_DOW_hour$Departure.Hour),]

#Remove departure hour 1
by_DOW_hour <- subset(by_DOW_hour,by_DOW_hour$Departure.Hour != 1)

ggplot(data= by_DOW_hour, aes(x=Departure.Hour, y=Avg.Delay, group = DOW, color = DOW)) + geom_line(colour = by_DOW_hour$DOW) + geom_point( size=4, shape=21, fill="white") + ggtitle("Delay time by DOW") + theme(plot.title = element_text(lineheight=1.2, face="bold"))
```

The day of the week trends show that throughout the year, daily delay patterns are fairly constant with delays increasing for flights scheduled to depart from 5 AM to 11 AM. The delay in flight plateaus till around 3 PM and starts increasing from 4 PM to 5:30 PM and drops back to an average delay of ~20 mins until 10 PM.

Hence, best time to fly would be either early in the morning before delays start peaking or late at night

```{r}
#Month level
by_month_hour <-aggregate(outbound_Aus_delay$DepDelay,by=list(outbound_Aus_delay$Month,outbound_Aus_delay$DepHour), FUN=mean, na.rm=TRUE)
#names(by_month_hour)
names(by_month_hour)[1] <- "Month"
names(by_month_hour)[2] <- "Departure.Hour"
names(by_month_hour)[3] <- "Avg.Delay"

by_month_hour <- by_month_hour[order(by_month_hour$Month, by_month_hour$Departure.Hour),]
by_month_hour <- subset(by_month_hour, by_month_hour$Departure.Hour != 1)

ggplot(data=by_month_hour, aes(x=Departure.Hour, y=Avg.Delay, group = Month, color = Month)) + geom_point( size= 5, shape=21, fill="white") + geom_line(colour = by_month_hour$Month) 
```

Departure by month show pretty much the same trends. Hence, let look at hourly departure delay by season: 

```{r}
#Season level
by_Season_hour <-aggregate(outbound_Aus_delay$DepDelay,by=list( outbound_Aus_delay$Season,outbound_Aus_delay$DepHour), FUN=mean, na.rm=TRUE)
names(by_Season_hour)
names(by_Season_hour)[1] <- "Season"
names(by_Season_hour)[2] <- "Departure.Hour"
names(by_Season_hour)[3] <- "Avg.Delay"

by_Season_hour <- by_Season_hour[order(by_Season_hour$Season, by_Season_hour$Departure.Hour),]
by_Season_hour <- subset(by_Season_hour, by_Season_hour$Departure.Hour != 1)

ggplot(data= by_Season_hour, aes(x=Departure.Hour, y=Avg.Delay, group = by_Season_hour$Season, color = by_Season_hour$Season)) + geom_line(aes(colour = by_Season_hour$Season)) + geom_point( size=4, shape=21, fill="white") + ggtitle("Delay time by Season") + theme(plot.title = element_text(lineheight=1.2, face="bold"))
```

Flights are delayed less in Fall than in other seasons. Flights scheduled to departure from 6AM till 10AM experience delay gradually. The delay plateaus and then increases in he evening from 4PM to 6PM and then reduces to 20 mins average around 10 PM 

```{r}
#Season and airlines level
by_Season_carrier_hour <-aggregate(outbound_Aus_delay$DepDelay,by=list( outbound_Aus_delay$Season,outbound_Aus_delay$DepHour,outbound_Aus_delay$UniqueCarrier), FUN=mean, na.rm=TRUE)
names(by_Season_carrier_hour)
names(by_Season_carrier_hour)[1] <- "Season"
names(by_Season_carrier_hour)[2] <- "Departure.Hour"
names(by_Season_carrier_hour)[3] <- "UniqueCarrier"
names(by_Season_carrier_hour)[4] <- "Avg.Delay"

by_Season_carrier_hour <- by_Season_hour[order(by_Season_carrier_hour$UniqueCarrier,by_Season_carrier_hour$Season, by_Season_carrier_hour$Departure.Hour),]
by_Season_carrier_hour <- subset(by_Season_carrier_hour, by_Season_carrier_hour$Departure.Hour != 1)


by_Season_carrier <-aggregate(outbound_Aus_delay$DepDelay,by=list( outbound_Aus_delay$Season,outbound_Aus_delay$UniqueCarrier), FUN=mean, na.rm=TRUE)
names(by_Season_carrier)
names(by_Season_carrier)[1] <- "Season"
names(by_Season_carrier)[2] <- "UniqueCarrier"
names(by_Season_carrier)[3] <- "Avg.Delay"

by_Season_carrier <- by_Season_carrier[order(by_Season_carrier$UniqueCarrier,by_Season_carrier$Season),]

ggplot(data= by_Season_carrier, aes(x=UniqueCarrier, y=Avg.Delay, group = Season, color = Season)) + geom_point( size=8, shape=2, fill="white") + ggtitle("Delay time by Season") + theme(plot.title = element_text(lineheight=1.2, face="bold"))
```

At a cursory glance, most of the airlines have a shorter departure delay in fall and longest in summer or winter.


##Model 1: Naive Bayes

Creating the training dataset:

```{r}
library(tm)

readerPlain = function(fname){
  readPlain(elem=list(content=readLines(fname)), 
            id=fname, language='en') }

#Training dataset

author_dirs = Sys.glob('../data/ReutersC50/C50train/*')

file_list = NULL
labels = NULL

for(author in author_dirs) {
  author_name = substring(author, first=29)
  files_to_add = Sys.glob(paste0(author, '/*.txt'))
  file_list = append(file_list, files_to_add)
  labels = append(labels, rep(author_name, length(files_to_add)))
}

all_docs = lapply(file_list, readerPlain) 
names(all_docs) = file_list
names(all_docs) = sub('.txt', '', names(all_docs))

train_corpus = Corpus(VectorSource(all_docs))
names(train_corpus) = file_list


train_corpus = tm_map(train_corpus, content_transformer(tolower))
train_corpus = tm_map(train_corpus, content_transformer(removeNumbers))
train_corpus = tm_map(train_corpus, content_transformer(removePunctuation))
train_corpus = tm_map(train_corpus, content_transformer(stripWhitespace))
train_corpus = tm_map(train_corpus, content_transformer(removeWords), stopwords("en"))

DTM_train = DocumentTermMatrix(train_corpus)
DTM_train = removeSparseTerms(DTM_train, .99)
DTM_train

X_train = as.matrix(DTM_train)
```

Running Naive Bayes on the training set:

```{r}
smooth_count = 1/nrow(X_train)
w = rowsum(X_train + smooth_count, labels)
w = w/sum(w)
w = log(w)
```

Creating the test dataset:

```{r}
# Test Dataset
author_dirs = Sys.glob('C:/Users/Dhwani/Documents/Coursework/Summer - Predictive Analytics/STA380/STA380/data/ReutersC50/C50test/*')
# author_dirs = author_dirs[1:2]
file_list = NULL
test_labels = NULL
author_names = NULL

for(author in author_dirs) {
  author_name = substring(author, first=106)
  author_names = append(author_names, author_name)
  files_to_add = Sys.glob(paste0(author, '/*.txt'))
  file_list = append(file_list, files_to_add)
  test_labels = append(test_labels, rep(author_name, length(files_to_add)))
}

all_docs = lapply(file_list, readerPlain) 
names(all_docs) = file_list
names(all_docs) = sub('.txt', '', names(all_docs))

test_corpus = Corpus(VectorSource(all_docs))
names(test_corpus) = file_list

test_corpus = tm_map(test_corpus, content_transformer(tolower))
test_corpus = tm_map(test_corpus, content_transformer(removeNumbers))
test_corpus = tm_map(test_corpus, content_transformer(removePunctuation))
test_corpus = tm_map(test_corpus, content_transformer(stripWhitespace))
test_corpus = tm_map(test_corpus, content_transformer(removeWords), stopwords("en"))

# make sure that we have all the words from training set
DTM_test = DocumentTermMatrix(test_corpus, list(dictionary=colnames(DTM_train)))
DTM_test

X_test = as.matrix(DTM_test)
```

Run prediction on test matrix:

```{r}
predict = NULL
for (i in 1:nrow(X_test)) {
  # get maximum Naive Bayes log probabilities
  max = -(Inf)
  author = NULL
  for (j in 1:nrow(w)) {
    result = sum(w[j,]*X_test[i,])
    if(result > max) {
      max = result
      author = rownames(w)[j]
    }
  }
  predict = append(predict, author)
}

predict_results = table(test_labels,predict)
correct = NULL
for (i in 1:nrow(predict_results)) {
  correct = append(correct, predict_results[i, i])
}

author.predict.correct = data.frame(author_names, correct)
author.predict.correct <- author.predict.correct[order(-correct),] 
author.predict.correct$per.correct <- author.predict.correct$correct/50

author.predict.correct

#Accuracy
sum(author.predict.correct$correct)/nrow(X_test)
```

The Naive Bayes algorithm gives us 60.24% accuracy. The model is unable to predict properly for the following authors - They have prediction accuracy of less than 25%:

DavidLawder
WilliamKazer
BenjaminKangLim
EdnaFernandes
ScottHillis
TanEeLyn

##Model 2: Random Forest

```{r}
#Adding words present in training but not in test, to our test dataframe for Random Forest to run

DTM_test = as.matrix(DTM_test)
DTM_train = as.matrix(DTM_train)
DTM_train_df = as.data.frame(DTM_train)

common <- data.frame(DTM_test[,intersect(colnames(DTM_test), colnames(DTM_train))])
all.train <- read.table(textConnection(""), col.names = colnames(DTM_train), colClasses = "integer")

library(plyr)
DTM_test_clean = rbind.fill(common, all.train)
DTM_test_df = as.data.frame(DTM_test_clean)

library(randomForest)
rf.model = randomForest(x=DTM_train_df, y=as.factor(labels), mtry=4, ntree=300)
rf.pred = predict(rf.model, data=DTM_test_clean)

rf.table = as.data.frame(table(rf.pred,labels))

#install.packages("caret", dependencies = TRUE)
library("caret")
rf.confusion.matrix = confusionMatrix(table(rf.pred,test_labels))
rf.confusion.matrix$overall
rf.confusion.matrix.df = as.data.frame(rf.confusion.matrix$byClass)
rf.confusion.matrix.df[order(-rf.confusion.matrix.df$Sensitivity),1:2]
```

Random Forest model shows an accuracy of 74.1% and has trouble predicting the following authors correctly:

JaneMacartney
MureDickie
TanEeLyn
WilliamKazer
ScottHillis

#3. Association Rule Mining

```{r}
library(arules)

no_col <- max(count.fields("groceries.txt", sep = "\t"))
groceries <- read.table("groceries.txt",sep="\t",fill=TRUE,col.names=1:no_col)
groceries$basketid <- rownames(groceries) 
#head(groceries)
#tail(groceries)
#class(groceries)

#install.packages("splitstackshape", dependencies = TRUE)
library(splitstackshape)
groceries.map <- cSplit(groceries, "X1", ",", direction = "long")

#groceries.list <- split(groceries, seq(nrow(groceries)))
groceries.list <- split(groceries.map$X1, f = groceries.map$basketid)
groceries.list <- lapply(groceries.list, unique)

## Cast this variable as a special arules "transactions" class.
groceries.trans <- as(groceries.list, "transactions")

# Now run the 'apriori' algorithm
# Look at rules with support > .01 & confidence >.3 & length (# grocery items) <= 20
grocrules <- apriori(groceries.trans, parameter=list(support=.005, confidence=.3, maxlen=8))
                         
# Look at the output
#inspect(grocrules)

## Choose a subset
inspect(subset(grocrules, subset=support > .05))
```

Products like yogurt, rolls/buns and other vegetables have a high support - which means these products are purchased more often that the other products.

```{r}
inspect(subset(grocrules, subset = confidence > 0.63))

```

Confidence is the ratio of the number of transactions that include all items in the consequent as well as the antecedent to the number of transactions that include all items in the antecedent.

Looking at products whose purchase likelihood increases by atleast 63% provided we purchased groceries in antecedent, we observe that a basket which contains fruits, root vegetables, yogurt, cream etc is morelikely to have whole milk as well. Hence, a store can work on its product placement strategy and place all the commonly purchased items like fruits, vegetables and dairy products together.

```{r}
inspect(subset(grocrules, subset=lift > 3))
#high lift and high support
inspect(subset(grocrules, subset=lift > 3 & support > .01))
```

Lift is the ratio of Confidence to Expected Confidence. In other words, Lift is a value that  gives us information about the increase in probability of the consequent given the antecedent part.

High lift indicates that relationship between antecedent and consequent is more significant that what would be expected if two sets were independent. Larger the lift, higher is the association.

For products which are purchased frequently (high support),like beef goes with root vegetables. Root vegetables,tropical fruits, citrus fruits and other vegetables go hand in hand.

```{r}
inspect(subset(grocrules, subset=support > .01 & confidence > 0.55))
```

The association exercise on grocery basket data can help a store manager place his products more accuratey, or maybe prescribe coupons based on the association rules. Since, dairy products, citrus fruits, vegetables go well together with whole milk, a store manager can place all these products together so that its easy for a shopper to pick his groceries.

Incase of a new whole milk brand launch, the manager can target the consumer who are more likely to purchase other products with coupons or marketing campiagn for the newer brand.
